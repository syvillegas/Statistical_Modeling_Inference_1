---
title: "Statistical Modelling and Inference - Seminar 2"
author: "Sergio-Yersi Villegas Pelegrín"
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 3
    number_sections: yes
---
```{r}
# We first load the required libraries in order to be able to run the code:
library(bayestestR)
library(rstanarm)
library(ggplot2)
library(tidyverse)
library(mombf)
library(mvtnorm)
library(glmnet)

```
# Load the Vessel data
```{r}
# We load the data that will be used
PATH <- '~/Desktop/TERM 1/SMI/seminar2'
data_logx <- log(read_csv(file.path(PATH, "data/Vessel_X.txt"), col_names = FALSE))
data_y <- read_csv(file.path(PATH, "data/Vessel_Y.txt"), col_names = FALSE)
data_y <- data_y[,1]
colnames(data_logx)<-sprintf("%s%i","F",seq(100,400,1))
colnames(data_y)<-sprintf("%s%i","Y",seq(1,ncol(data_y)))
data<-cbind(data_logx,data_y)

```
# Conduct a prior eliciation to choose a value of g for the regression of compound 1 content on the 301 frequencies

The strategy that has been used to choose the value of $g$ is related to the obtained value of the linear regression $τ$ coefficient, which corresponds to the theoretical $R^2$ coefficient. In this case, the main objective is that this coefficient does not have a strong explanatory power and thus, providing minimally sensible prior beliefs. Hence, we must choose a value of $g$ that gives a low value of the theoretical $R^2$ coefficient, corresponding to the prior proportion of variance explained. By noticing how the theoretical $R^2$ changes with $g$, as we can see in the following figures, we will determine the best value of $g$.

```{r}
# We define the design matrix X and the y_data
X <- scale(as.matrix(data_logx))
y <- data_y

# We define two fixed grids of values for g
# in order to be able to compare them.
low_g = exp(seq(log(.00001),log(.00002),length=10))
high_g = exp(seq(log(.1),log(.2),length=10))
n = nrow(data)

V = diag(ncol(X))
beta = rmvnorm(1000, sigma= V)
sse = colSums((X %*% t(beta))^2) / n

# We define two theoretical R^2 variables
# which will be accordingly used with each g.
tr_squared_low = double(length(low_g))
tr_squared_high = double(length(high_g))

# We compute the sequence of values for both
# theoretical coefficients.
for (i in 1:length(low_g)) {
  tr_squared_low[i]= mean(sse * low_g[i] / (1 + sse * low_g[i]))
}

for (i in 1:length(high_g)) {
  tr_squared_high[i]= mean(sse * high_g[i] / (1 + sse * high_g[i]))
}

# We plot both high and low cases, so that
# we can compare them and choose the right value of g.
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(low_g, tr_squared_low, type='l', xlab='g', ylab=expression(paste('Theoretical ',R^2)))

par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(high_g, tr_squared_high, type='l', xlab='g', ylab=expression(paste('Theoretical ',R^2)))
```
For values of $g$ $\sim$ $10^{-1}$, we would have a 95% of explained variance, while for values of $g$ $\sim$ $10^{-5}$, the obtained theoretical $R^2$ would provide an explanatory power inferior to a 1%. Hence, as we want to keep prior beliefs with the smallest possible effects, we would choose the lowest values of $g$. In this case, $g$ $\sim$ $10^{-5}$.

# Run a Bayesian model selection

We run the Bayesian model selection implemented in the function $modelSelection()$ for the Vessel data. After we run the model selection, as it can be seen in the following results, we will print the top 10 models with their corresponding posterior probabilities, where the list of variables included in each model corresponds to the $modelid$ column. 

```{r, results='hide'}
# Run the Bayesian Model Selection and assign the results to a variable
fit.bayesreg <- modelSelection(y=y$Y1,x=X, priorCoef=zellnerprior(taustd=1), priorDelta=modelbbprior(1,1))

```
```{r}
# Print the top 10 models obtained
head(postProb(fit.bayesreg),10)

```
Then, with the function $coef()$ we can obtain information about each predictor: the Bayesian model averaging estimates for each coefficient, the posterior intervals and the posterior marginal inclusion probabilities. In order to determine the number of predictors that will be used, the reasonable criterion would be to choose those with a non-zero coefficient. However, since each predictor has its corresponding posterior marginal inclusion probability, one could argue that only those with a posterior marginal inclusion probability higher than a minimally relevant value should be chosen, since the ones with very low posteriors only differ from 0 in a very small order. For example, using the above results, we can see that for the top 1 model, the number of selected variables is equal to 8. 

Nevertheless, by computing the number of predictors with a non-zero coefficient, we would get around 140 relevant predictors (since each time you run the code, the number may slightly change due to the probabilistic approach of the model). We will later discuss this issue in section 5, when comparing the Bayesian regression with LASSO.
```{r}
# Get the relevant information of each coefficient.
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
head(ci.bayesreg,5)
# Get the predictors that have a non-zero coefficient
# in order to obtain the number of predictors that
# will be selected
sel.bayesreg <- ci.bayesreg[,1] != 0
bayes_vars=0
for (i in 1:length(sel.bayesreg)){
  if (sel.bayesreg[i]==TRUE){
    bayes_vars=bayes_vars+1
}
}
cat("Number of non-zero coefficients =",bayes_vars)
cat("Number of selected variables in the top 1 model =",8)
```

With this information, we can plot the confidence intervals of all the variables, where the ones that differ from 0 correspond to the most relevant predictors of the model. There are many points that will not be appreciated in the figure, since they differ from 0 in a very small fraction. 

```{r}
# We plot the confidence intervals of the predictors:
# the ones that differ from 0 are the ones that have
# a significant contribution to the model.
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI',
     xlab='Index of predictor', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:nrow(ci.bayesreg), ci.bayesreg[, 1], pch = 16)
```
# Obtain predictions by Bayesian Model Averaging
In order to obtain the right predictions for the Bayesian regression obtained through the Bayesian Model Selection, we must take into account the value of the intercept, which can be extracted from the first coefficient of the model results. Then, once we get our predictions with the $predict()$ function, we compute the $R^2$ coefficient and we obtain a value of: 
```{r}
intercept <- coef(fit.bayesreg)[1,1]
pred_bayes <- predict(fit.bayesreg) + intercept
# Now, we can obtain the correlation coefficient between our predictions and the given observations of y:
r_squared_bayes <- (cor(y,pred_bayes[,1]))^2
cat("R^2 =",r_squared_bayes)

```
# Compare to the results obtained with LASSO
Now, to conclude this report, we will compare the obtained results with the corresponding results obtained through the LASSO from Seminar 1. In order to compare both models, we will focus on the number of selected predictors of each model, their $R^2$ value and the computational time that each model takes.

First, we fit the model for LASSO in order to obtain the number of variables we will be considering, which are the ones with a non-zero coefficient. We have used the same function from Seminar 1 (from the glmnet package) and we have obtained a total number of variables with non-zero coefficients equal to 32. We will later compare this result to the one obtained for Bayes, returning to the discussed issue in section 3.

```{r}
# We fit the model for the gaussian family distribution
# using the cv.glmnet function via the 10-fold cross-validation
# from Seminar 1.
fit.lassocv = cv.glmnet(x=X, y=y$Y1, nfolds=10, 
                        family='gaussian')

# In order to determine the number or variables, or predictors,
# that we will have, we count the number of coefficients with a
# non-zero value.
coef_lassocv <- as.vector(coef(fit.lassocv, s='lambda.min'))
names(coef_lassocv) <- c('intercept', colnames(data_logx))
lassocv_vars = length(coef_lassocv[coef_lassocv!=0])
cat("Number of selected predictors =",lassocv_vars)
```
Now, we will simultaneously compute the $R^2$ coefficient and the computational required time through a K-fold cross-validation for both types of regression models: Bayes and LASSO. We have used the following function from Seminar 1, which has been accordingly modified to accept a Bayesian criterion (more detailed in the code's comments below).
```{r}
# We have modified the function from Seminar 1 by adding one extra 
# 'elseif', for the case in which we want to use a Bayesian criterion
# to do the cross-validation.

kfoldCV.sem_2 <- function(y,x,K,seed,criterion) {
## Perform K-fold cross-validation for LASSO regression estimate (lambda set either via cross-val or BIC or EBIC)
## Input
## - y: response
## - x: data.frame with predictors, intercept should not be present
## - K: number of folds in K-fold cross-validation
## - seed: random number generator seed (optional)
## - criterion: the criterion to select the penalization parameter, either cross-val or BIC or EBIC
## Output
## - pred: cross-validated predictions for y
## - ssr: residual sum of squares, sum((y-pred)^2)
  require(glmnet)
  if (!missing(seed)) set.seed(seed)
  subset <- rep(1:K,ceiling(nrow(x)/K))[1:nrow(x)]
  subset <- sample(subset,size=nrow(x),replace=FALSE)
  pred <- double(nrow(x))
  cat("Starting cross-validation")
  if (ncol(x)>0) {  #if there are some covariates
    for (k in 1:K) {
        sel <- subset==k
        if (criterion=='cv') {
            fit <- cv.glmnet(x=x[!sel,,drop=FALSE], y=y[!sel], alpha = 1, nfolds=10)
            b= as.vector(coef(fit,s='lambda.min'))
            pred[sel] <- b[1] + x[sel,,drop=FALSE] %*% as.matrix(b[-1])
        } else if (criterion=='bic'){
            fit <- lasso.bic(y=y[!sel],x=x[!sel,,drop=FALSE])
            pred[sel] <- fit$coef[1] + x[sel,,drop=FALSE] %*% matrix(fit$coef[-1],ncol=1)
        } else if (criterion=='ebic'){
            fit <- lasso.bic(y=y[!sel],x=x[!sel,,drop=FALSE],extended = TRUE)
            pred[sel] <- fit$coef[1] + x[sel,,drop=FALSE] %*% matrix(fit$coef[-1],ncol=1) 
        } else if (criterion=='bayes'){
            fit.bayesreg <- modelSelection(y=y[!sel],x=x[!sel,,drop=FALSE], priorCoef=zellnerprior(taustd=1), priorDelta=modelbbprior(1,1))
            intercept <- coef(fit.bayesreg)[1,1]
            pred[sel] <- predict(fit.bayesreg, newdata = x[sel,,drop=FALSE])[,1] + intercept
        } else { stop("method.lambda not implemented") }
        cat(".")
    }
  } else { #if there are no covariates, just use the intercept
    for (k in 1:K) {
      sel <- subset==k
      pred[sel] <- mean(y[!sel],na.rm=TRUE)
    }
  }
  cat("\n")
  return(list(pred=pred,ssr=sum((pred-y)^2,na.rm=TRUE)))
}
```

Firstly, we perform the K-folds for a Bayesian regression, extract the obtained predictions and compute the corresponding $R^2$. Meanwhile, we have been computing the required time to run the code, and the results are the following: 

```{r ,results='hide'}
# We set the seed
seed=2345
# Call and assign the function to a variable names 'lasso_bayes'
start.time.b <- Sys.time()
kfolds_bayes_cv = kfoldCV.sem_2(x=X, y=y$Y1,K=10,seed=seed,
                          criterion="bayes")
end.time.b <- Sys.time()
time.taken.b <- end.time.b - start.time.b
```

```{r}
# We get the predictions obtained, in order to be able to compute
# the R^2 coefficient.
predictions_bayes_cv <- kfolds_bayes_cv$pred
r_squared_bayes_cv <- cor(y,predictions_bayes_cv)^2
cat("Bayesian cross-validated R^2 =", r_squared_bayes_cv)
cat("Computational time for a Bayessian regression: ", time.taken.b, "seconds")
```

Secondly, we do the same for a LASSO regression: compute the corresponding $R^2$ and the required time to run the code. For LASSO, the results are:

```{r, results='hide'}
start.time.l <- Sys.time()
kfolds_lasso_cv = kfoldCV.sem_2(x=X, y=y$Y1,K=10,seed=seed,
                          criterion="cv")
end.time.l <- Sys.time()
time.taken.l <- end.time.l - start.time.l
```
```{r}
predictions_lasso_cv <- kfolds_lasso_cv$pred
r_squared_lasso_cv <- cor(y,predictions_lasso_cv)^2
cat("LASSO cross-validated R^2 =",r_squared_lasso_cv)
cat("Computational time for a LASSO regression: ", time.taken.l, "seconds")
```

After having gone through the most relevant features of both models, let's compare all the obtained results. First, we can see practically equal values of the $R^2$ coefficient for both regression models. Then, when talking about required computational time, we can see that for a LASSO regression, the code runs around 45 times faster than for a Bayesian regression. Finally, regarding the number of selected variables by each model, we will return to the issue we discussed in section 3. 

In order to do a fair comparison with LASSO, we should unify the selection criterion of variables for both models, and therefore select those with a non-zero coefficient for the Bayesian regression. Hence, using the same criterion, LASSO would have a significantly lower number of selected predictors (32) in comparison with the ones from the Bayesian regression (around 140). However, as we saw, the number of selected variables for a Bayesian regression depends on the posterior marginal inclusion probability of each predictor, and one could just take into account those that are high enough to be relevant. Therefore, the number of selected predictors for each model are not perfectly comparable, due to the fact that Bayes has a probabilistic approach and even though it has, strictly speaking, a much higher number of selected predictors, many of them will barely have an effect on the predictions. Hence, the number of "truly" relevant predictors would be much more close to the one for LASSO, and would then depend on the chosen threshold. 

In conclusion, both models have their own particular features, both with high $R^2$ values, and therefore could be perfectly used. Since they are both based in different approaches, one should analyze the problem to solve and then choose which model may be fundamentally better for that particular case.  



